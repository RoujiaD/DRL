from __future__ import absolute_import, division, print_function, unicode_literals
import gym
import numpy as np
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import matplotlib.pyplot as plt
import math

STATE_SHAPE = (4,)
ACTION_SIZE = 2
BATCG_SIZE = 1500
LEARNING_RATE = 0.001
EPOCHS = 600
number = 50000

def baseline_model():
    model = Sequential()
    model.add(Dense(64, input_shape=STATE_SHAPE, activation="relu"))
    model.add(Dense(64, activation="relu"))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer=Adam(lr=LEARNING_RATE))
    return model

def smaller_model():
    model = Sequential()
    model.add(Dense(10, input_shape=STATE_SHAPE, activation="relu"))
    model.add(Dense(10, activation="relu"))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer=Adam(lr=LEARNING_RATE))
    return model

def bigger_model():
    model = Sequential()
    model.add(Dense(400, input_shape=STATE_SHAPE, activation="relu"))
    model.add(Dense(400, activation="relu"))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer=Adam(lr=LEARNING_RATE))
    return model

def plot_history(histories, key='loss'):
  plt.figure()

  for name, history in histories:
    val = plt.plot(history.epoch, history.history['val_'+key],
                   '--', label=name.title()+' Val')
    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),
             label=name.title()+' Train')

  plt.xlabel('Epochs')
  plt.ylabel(key.replace('_',' ').title())
  plt.legend()

  plt.xlim([0,max(history.epoch)])


def check(x, rewards, states):
  #   Check ternimal conditions: |3rd element| > 12 degree? or |1st element| >2.4?
  b = [a for a in range(len(rewards)) if rewards[a] == x]
  for i in range(len(b)):
    if abs(states[b[i]][0]) <= 2.4 and abs(states[b[i]][2]) <= 12 * 2 * math.pi / 360:
      print("State doesn't match reward")
      break


def calMSE(pred, output):
  return sum((pred[i] - output[i])**2 for i in range(len(output)))


def main():

    states = np.zeros((number, 4))
    rewards = []

    # Create and reset the Cartpole env:
    env = gym.make('CartPole-v1')
    state = env.reset()
    # Generate training & test set
    for i in range(number):
      # Random action
        action = env.action_space.sample()
        next_state, reward, is_terminal, _ = env.step(action)
        states[i] = next_state
        state = next_state

        if is_terminal:
            reward = -100
            state = env.reset()
        rewards.append(reward)

    # Check and split date set into training and testing
    check(-100, rewards, states)
    train_states = states[0:round(number*0.8)]
    train_output = rewards[0:round(number*0.8)]
    test_states = states[round(number*0.8)+1:number]
    test_output = rewards[round(number*0.8)+1:number]
    base_model = baseline_model()
    small_model = smaller_model()
    big_model = bigger_model()
    base_his=base_model.fit(train_states, train_output, epochs=EPOCHS,batch_size=BATCG_SIZE,
                            validation_split=0.2, verbose=2)
    small_his=small_model.fit(train_states,train_output,epochs=EPOCHS,batch_size=BATCG_SIZE,
                              validation_split=0.2, verbose=2)
    big_his = big_model.fit(train_states, train_output, epochs=EPOCHS, batch_size=BATCG_SIZE,
                              validation_split=0.2, verbose=2)

    plot_history([('baseline', base_his), ('smaller', small_his), ('bigger', big_his)])

    pred = big_model.predict(test_states)
    error = pred - test_output
    plt.figure()
    plt.hist(error)
    plt.xlabel('Prediction Error-bigger model')
    _ = plt.ylabel('Count')


if __name__ == "__main__":
    main()




